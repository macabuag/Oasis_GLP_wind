{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oasis model for Guadaloupe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: \n",
    "Silvia Bertelli\n",
    "\n",
    "#### Date \n",
    "27th October 2020\n",
    "    \n",
    "#### Decription:\n",
    "Jupyter notebook for running the Oasis model:\n",
    "- estimates the windfiled using the STORM DATASET;\n",
    "- adopted the GAR exposures files ad convert them into the OED format;\n",
    "- convert the GAR vulnerability functions into the oasis format;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Case study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook build and run an oasis model for Guadaloupe (GLP), which is located (WGS84):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = 16.1730949\n",
    "longitude = -61.4054001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projected coordinate system for the area is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectedsystem = \"EPSG:2970\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, check here the reference for the adopted coordinate system: https://epsg.io/2970 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, specify a buffer radius [m] of the extent of the administrative bundaries in order to define the assessed area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiusbuffer = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the resolution of a grid through the following width and lenght values [m]; for this model, the grid is a simple uniform grid in squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_width = 10000\n",
    "res_height = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, select a distance [km] (estimated from the centroid of the country) and wind category to select events from the STORM dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_wind = 25\n",
    "cat_wind = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Python code set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.1 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import json\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from shapely.geometry import Polygon, MultiPoint, Point, box\n",
    "from descartes import PolygonPatch\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.offsetbox import AnchoredText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the python functions for the windstorm calculations and to convert files to the Oasis input format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oasismodel_functions as omf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 2.2 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check which is the current working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd()) #this is my current working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set this folder as the current working directory. You may want to comment (#) the following block if the two folders corresponds already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGE_DIR = os.path.abspath(os.path.join(os.path.dirname('__file__'), '..'))\n",
    "#print(\"Current working directory is:\" + PACKAGE_DIR)\n",
    "#sys.path.append(PACKAGE_DIR)\n",
    "#FILE_DIR = os.path.dirname('__file__')\n",
    "#FILE_DATA_DIR = os.path.join(TEST_DIR, 'inputs/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Input data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin = '../model_data/admin/gadm36_GLP_0.shp'\n",
    "gar = '../model_data/gar/lac_glp.shp'\n",
    "intensity_dic = '../model_data/PiWind/intensity_bin_dict.csv'\n",
    "damage_dic = '../model_data/PiWind/damage_bin_dict.csv'\n",
    "fb_df_vf = '../model_data/vulnerability/GLP_Vulnerability_uniquevaluestest.csv' # vulnerability database \n",
    "fp_gar_dat = '../model_data/vulnerability/VulWdLAC.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Output data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script produces the following shapefiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_grid = '../tests/outputs/shp/grid.shp'\n",
    "fp_wind = '../tests/outputs/shp/wind_dataset.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the following csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_areaperil = '../keys_data/PiWind/areaperil_dict.csv'\n",
    "fp_events = '../model_data/PiWind/events_p.csv'\n",
    "fp_occurrence ='../model_data/PiWind/occurrence_lt.csv'\n",
    "fp_footprint = '../model_data/PiWind/footprint.csv'\n",
    "fp_vf_dic = '../keys_data/PiWind/vulnerability_dict.csv'\n",
    "fp_vulnerability = '../model_data/PiWind/vulnerability.csv'\n",
    "fp_exp2 = '../tests/inputs/SourceLocOED.csv'\n",
    "\n",
    "fp_storm_csv = '../model_data/storm/STORM_DATA_IBTRACS_NA.csv'\n",
    "\n",
    "path_vf = 'outputs/vulnerability_functions/{}.csv' #chech this\n",
    "fp_vf_by_name = '../model_data/vulnerability/vf_by_name/{}.csv'\n",
    "path_vf_out = '../tests/outputs/vulnerability/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Assessed area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the administrative boundaries from the GADM shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_admin = omf.get_admin(admin, projected_crs = projectedsystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer the administrative bundaries in order to define a larger area to be assessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_admin_buffer = omf.buffer_grid(gdf_admin, radius = radiusbuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and draw a grid over the assessed area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_grid = omf.get_grid(gdf_admin_buffer, height=res_height, width=res_width, projected_crs = projectedsystem, fp_grid = fp_grid)\n",
    "print(\" A shapefile of the grid has been saved into \" + str(fp_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's determine the centroids and the vertices of this grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_centroids = omf.get_grid_centroid(gdf_grid, projected_crs = projectedsystem)\n",
    "gdf_vertices = omf.get_grid_vertices(gdf_grid,projected_crs = projectedsystem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the grid into the oasis format and save it as csv file (TODO: convert the coverage into a list at the top of the notebook and loop through them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_peril_df1 = omf.convert_to_oasis_areaperil(gdf_vertices, peril ='WTC', coverage = 1)\n",
    "area_peril_df2 = omf.convert_to_oasis_areaperil(gdf_vertices, peril ='WTC', coverage = 3)\n",
    "frames_areaperil = [area_peril_df1, area_peril_df2] #add here all the areaperils dataframes (we have different df as we have different coverages and perils)\n",
    "area_peril_dictionary = omf.concatenate_df_areaperil(frames_areaperil, fp_areaperil)\n",
    "print(\" A csv of the area_peril_dictionary has been saved into \" + str(fp_areaperil))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the model grid data contained in the Area Peril dictionary file. \n",
    "Note that the dictionary is only meta-data, and not required for model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_peril_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot the area peril cells on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = omf.areapeeril_map(area_peril_dictionary,latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=350 src=\"extent_map.html\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Windfield estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Selection of events from the STORM dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the STORM dataset and select events based on the distance and category of the event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storm = omf.load_multiple_STORM(gdf_admin, distance_wind, cat_wind, fp_storm_csv) \n",
    "df_storm.info()\n",
    "print(\" A csv of the selected windtrack has been saved into \" + str(fp_storm_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the B-shape parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_windtrack = omf.STORM_analysis(df_storm)\n",
    "df_windtrack.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of events that have been selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_events = omf.unique_events(df_windtrack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and convert them into a shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wind = omf.merge_unique_events_windtrack(list_events, df_windtrack, fp_wind)\n",
    "print(\" A shapefile of the selected events has been saved into \" + str(fp_wind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, convert the list of events into the oasis format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oasis_events = omf.oasis_events(list_events, fp_events)\n",
    "df_oasis_events.head()\n",
    "print(\" A csv of events dataframe in the oasis format has been saved into \" + str(fp_events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data contained in the occurence dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oasis_occurence = omf.oasis_occurence(list_events, fp_occurrence)\n",
    "df_oasis_occurence.head()\n",
    "print(\" A csv of the occurence dataframe in the oasis format has been saved into \" + str(fp_occurrence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the data contained in the Intensity Bin dictionary file. \n",
    "Note that the dictionary is only meta-data, and not required for model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_bin_dictionary = omf.load_intensity(intensity_dic) #TODO: create the dataframe base on the windtrack dataframe\n",
    "print(intensity_bin_dictionary.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = omf.merge_wind_w_grid(df_wind, gdf_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = omf.groupby_grid(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup = omf.intensity_lookup(intensity_bin_dictionary, df_groupby)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the data contained in the footprint file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprints = omf.get_footprint(df_lookup, fp_footprint)\n",
    "footprints.head()\n",
    "print(\" A csv of the occurence dataframe in the oasis format has been saved into \" + str(fp_footprint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the first 5 event footprints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "area_peril_dictionary['gridcell'] = area_peril_dictionary['AREA_PERIL_ID'].apply(\n",
    "    lambda ap: str(int((ap-1)/10)+1)+\"-\"+str(ap-(int((ap-1)/10))*10))\n",
    "\n",
    "footprints_with_hazard = footprints.merge(\n",
    "    intensity_bin_dictionary, how='inner', \n",
    "    left_on='intensity_bin_id', right_on='bin_index').merge(\n",
    "    area_peril_dictionary, how='inner', \n",
    "    left_on='areaperil_id', right_on='AREA_PERIL_ID')\n",
    "\n",
    "footprints_with_hazard = footprints_with_hazard[footprints_with_hazard['PERIL_ID']=='WTC']\n",
    "footprints_with_hazard = footprints_with_hazard[footprints_with_hazard['COVERAGE_TYPE']==1]\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols=(1, 5),\n",
    "                axes_pad=0.05,\n",
    "                share_all=True,\n",
    "                label_mode=\"L\",\n",
    "                cbar_location=\"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                )\n",
    "\n",
    "vmin = min(footprints_with_hazard.interpolation)\n",
    "vmax = max(footprints_with_hazard.interpolation)\n",
    "for idx, ax in enumerate(grid):\n",
    "    a = np.zeros([10, 10])\n",
    "    for __, row in footprints_with_hazard[footprints_with_hazard.event_id == idx+1].iterrows():\n",
    "        i, j = row.gridcell.split('-')\n",
    "        a[10-int(i), int(j)-1] = row.interpolation\n",
    "    im = ax.imshow(a, cmap=plt.cm.get_cmap('Blues'), vmin=vmin, vmax=vmax,\n",
    "                   extent=(\n",
    "                       min(area_peril_dictionary.lon), max(area_peril_dictionary.lon), \n",
    "                       min(area_peril_dictionary.lat), max(area_peril_dictionary.lat)))\n",
    "    ax.set_xlabel(\"longitude\")\n",
    "    ax.set_ylabel(\"latitude\")\n",
    "    at = AnchoredText(\n",
    "        \"Event ID = {}\".format(idx + 1),\n",
    "        prop=dict(size=8),\n",
    "        frameon=True,\n",
    "        loc=2,\n",
    "    )\n",
    "    at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    ax.add_artist(at)\n",
    "\n",
    "grid[0].cax.colorbar(im)\n",
    "cax = grid.cbar_axes[0]\n",
    "axis = cax.axis[cax.orientation]\n",
    "axis.label.set_text(\"Intensity - Peak gust (mph)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Vulnerability assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Damage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the data contained in the Damage Bin dictionary file. \n",
    "Note that the dictionary is required for model execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_bin_dictionary = omf.load_damage_dic(damage_dic)\n",
    "damage_bin_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vulnerability Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the vulnerability dataframe and save each road separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vf = omf.load_vf(fb_df_vf, fp_vf_by_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot each vulnerability function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_glp_vf_1 = omf.plot_multi_vf(path_vf_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Vulnerability dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the gar vulnerability dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gar = omf.read_gar_vf(fp_gar_dat)\n",
    "df_gar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dictionary file to link the vulnerability database with the gar dat file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gar_dictionary = omf.gar_dict(df_gar, df_vf)\n",
    "gar_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, define the vulnerability_dictionary file with the format required by the oasis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerability_dictionary = omf.vulnerability_dic(df_vf, gar_dictionary,fp_vf_dic)\n",
    "vulnerability_dictionary.head()\n",
    "print(\" A csv of the vulnerability_dictionary dataframe in the oasis format has been saved into \" + str(fp_vf_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Vulnerability file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the selected vulnerability functions based on the intensity_dictionary and damage_dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lookup_2 = omf.damage_intensity_lookup(df_glp_vf_1, damage_bin_dictionary, intensity_bin_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's format the results into the oasis format. Lets have a look at the data contained in the obrained Vulnerability file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerabilities = omf.get_vulnerability(df_lookup_2, fp_vulnerability)\n",
    "print(\" A csv of the vulnerability dataframe in the oasis format has been saved into \" + str(fp_vulnerability))\n",
    "vulnerabilities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has seperate vulnerability curves for Residential, Commerical and Industrial occupancies. \n",
    "Lets visualise these curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerabilities_with_hazard_and_damage = vulnerabilities.merge(\n",
    "    intensity_bin_dictionary, how='inner', \n",
    "    left_on='intensity_bin_id', right_on='bin_index').merge(\n",
    "    damage_bin_dictionary, how='inner',\n",
    "    suffixes=[\"_i\", \"_d\"], left_on='damage_bin_id', right_on='bin_index')\n",
    "\n",
    "fig = plt.figure(figsize=(10,20))\n",
    "\n",
    "grid = AxesGrid(fig, 111,\n",
    "                nrows_ncols=(1, 3),\n",
    "                axes_pad=0.05,\n",
    "                share_all=True,\n",
    "                label_mode=\"L\",\n",
    "                cbar_location=\"right\",\n",
    "                cbar_mode=\"single\",\n",
    "                )\n",
    "\n",
    "vmin = 0.0\n",
    "vmax = max(vulnerabilities_with_hazard_and_damage.probability)\n",
    "labels = [\"Residential\", \"Commercial\", \"Industrial\"]\n",
    "for idx, ax in enumerate(grid):\n",
    "    a = np.zeros((29, 12))\n",
    "    for index, row in vulnerabilities_with_hazard_and_damage[\n",
    "        vulnerabilities_with_hazard_and_damage.vulnerability_id == idx + 1].iterrows():\n",
    "        a[int(row.bin_index_i-1), 11-int(row.bin_index_d-1)] = row.probability\n",
    "    \n",
    "    im = ax.imshow(a, cmap=plt.cm.get_cmap('Blues'), vmin=vmin, vmax=vmax,\n",
    "                   extent=(\n",
    "                       min(intensity_bin_dictionary.interpolation), max(intensity_bin_dictionary.interpolation), \n",
    "                       min(damage_bin_dictionary.interpolation) * 100, max(damage_bin_dictionary.interpolation) * 100))\n",
    "\n",
    "    at = AnchoredText(labels[idx],\n",
    "                  prop=dict(size=8), frameon=True,\n",
    "                  loc=2,\n",
    "                  )\n",
    "    at.patch.set_boxstyle(\"round,pad=0.,rounding_size=0.2\")\n",
    "    ax.add_artist(at)\n",
    "    \n",
    "    ax.set_xlabel(\"Intensity - Peak gust (mph)\")\n",
    "    ax.set_ylabel(\"Damage\")\n",
    "\n",
    "grid[0].cax.colorbar(im)\n",
    "cax = grid.cbar_axes[0]\n",
    "axis = cax.axis[cax.orientation]\n",
    "axis.label.set_text(\"Probability of damage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Exposure Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model we need some test exposure data. Lets have a look at an example Location and Account file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_gar = omf.get_exposure(gar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_gar_location = omf.get_location(gdf_gar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_gar_occupancy = omf.lookup_occupancycode(gar_dictionary, gdf_gar_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_locations = omf.convert_to_oasis_exposure(gdf_gar_occupancy, \n",
    "                                               PortNumber = 1,\n",
    "                                               AccNumber= 'A11111',\n",
    "                                               IsTenant = 1,\n",
    "                                               BuildingID = 1,\n",
    "                                               ConstructionCode = 5000,\n",
    "                                               LocPerilsCovered = 'WTC',\n",
    "                                               ContentsTIV = 0,\n",
    "                                               BITIV = 0,\n",
    "                                               CondNumber = 0,\n",
    "                                               fp = fp_exp2\n",
    "                                              )\n",
    "print(\" A csv of the Exposure dataframe in the oasis format has been saved into \" + str(fp_exp2))\n",
    "test_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accounts = pd.read_csv('../tests/inputs/SourceAccOEDPiWind.csv')\n",
    "test_accounts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Oasis Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Analysis settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model, we also need to define some analysis settings. Lets have a look at an example settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../analysis_settings.json', 'r') as myfile:\n",
    "    analysis_settings=json.loads(myfile.read().replace('\\n', ''))\n",
    "print(json.dumps(analysis_settings, indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the model using the Oasis MDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Files to bin convertions and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, convert the csv file into the required binary format; and then validate them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! evetobin < ../model_data/PiWind/events_p.csv > ../model_data/PiWind/events_p.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! footprinttobin -i 29 <../model_data/PiWind/footprint.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! validatefootprint < ../model_data/PiWind/footprint.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp footprint.bin ../model_data/PiWind/ \n",
    "! cp footprint.idx ../model_data/PiWind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! vulnerabilitytobin -d 11 < ../model_data/PiWind/vulnerability.csv > ../model_data/PiWind/vulnerability.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! validatevulnerability < ../model_data/PiWind/vulnerability.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! occurrencetobin -P896 < ../model_data/PiWind/occurrence_lt.csv > ../model_data/PiWind/occurrence_lt.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! returnperiodtobin < ../model_data/PiWind/returnperiods.csv > ../model_data/PiWind/returnperiods.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! crossvalidation -d ../model_data/PiWind/damage_bin_dict.csv -f ../model_data/PiWind/footprint.csv -s ../model_data/PiWind/vulnerability.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/analysis_test\n",
    "! oasislmf model run -C ../oasislmf.json -r ../tests/outputs/analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize the output of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_directory = \"../tests/outputs/analysis\"\n",
    "gul_aep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_aep.csv\"))\n",
    "gul_aep = gul_aep[gul_aep.type == 1]\n",
    "gul_oep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_oep.csv\"))\n",
    "gul_oep = gul_oep[gul_oep.type == 1]\n",
    "eps = pd.merge(gul_oep, gul_aep, on=[\"summary_id\", \"return_period\"], suffixes=[\"_oep\", \"_aep\"])\n",
    "eps = eps.sort_values(by=\"return_period\", ascending=True)\n",
    "fig, ax = plt.subplots()\n",
    "eps.plot(ax=ax, kind='bar', x='return_period', y=[\"loss_oep\", \"loss_aep\"])\n",
    "ax.set_yticklabels(['{:,}'.format(int(x)) for x in ax.get_yticks().tolist()])\n",
    "ax.set_xticklabels(['{:,}'.format(int(x)) for x in eps.return_period])\n",
    "plt.legend(('OEP', 'AEP'))\n",
    "ax.set_xlabel(\"Return period (years)\")\n",
    "ax.set_ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
